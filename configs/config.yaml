model_name: meta-llama/Llama-3.2-1B
dataset_name: wikitext
dataset_dir: ./dataset
test_batch_size: 16
max_seq_len: 512
calib_size: 512 # change to 512

# pruning parameters
width_hidden: 0.5
width_intermediate: 0.5
width_attn: 0.0
batch_agg_func: l2

# training parameters
train_batch_size: 16  # number of independent sequences that'll be processed in parallel
block_size: 128  # maximum context length for the preds
max_iters: 1000
eval_interval: 200
learning_rate: 3e-4
eval_iters: 200
n_embd: 256
n_head: 4
n_blocks: 4
dropout: 0.2